{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "summer_project_nns.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP1TBGdCWbdzb14YyqTDwyn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheodorIvanov/Summer_Project/blob/master/summer_project_nns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j227QeM-oVCq",
        "colab_type": "text"
      },
      "source": [
        "## Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT8tg5dIoNb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files # insert json token\n",
        "\n",
        "import json # alternatively can use files.upload()\n",
        "dict = {\"username\":\"tedbg41\",\"key\":\"e8b997c78473e26b4bec20ca33c9ca66\"}\n",
        "with open('kaggle.json', 'w') as fp:\n",
        "    json.dump(dict, fp)\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# Downloading the Zip database from Kaggle\n",
        "!kaggle datasets download -d mlg-ulb/creditcardfraud\n",
        "\n",
        "# Unzipping the file\n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"creditcardfraud.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94JuY_dloaRj",
        "colab_type": "text"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU1TrqpjocYE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import gridspec\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set_palette(\"Set2\")\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.layers import *\n",
        "from keras import losses\n",
        "from keras import optimizers\n",
        "from keras import regularizers\n",
        "from keras.models import Model, load_model\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard, EarlyStopping\n",
        "\n",
        "seed = 66\n",
        "\n",
        "# Supressing  Warnings\n",
        "\n",
        "import warnings\n",
        "def fxn():\n",
        "    warnings.warn(\"deprecated\", DeprecationWarning)\n",
        "\n",
        "# Functions\n",
        "\n",
        "def LossGraph(history):\n",
        "  plt.figure(figsize = (12,9))\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['train', 'test'], loc='upper right')\n",
        "  plt.show();\n",
        "\n",
        "def ReconstructionScatter(cleared, frauds, threshold): \n",
        "  plt.figure(figsize = (12,9))\n",
        "  plt.scatter(cleared.index, cleared.values, marker = 'o', label = 'Cleared')\n",
        "  plt.scatter(frauds.index, frauds.values, marker = 'x', label = 'Fraud')\n",
        "  plt.hlines(threshold, plt.xlim()[0], plt.xlim()[1], colors = 'r', label = 'Threshold')\n",
        "  plt.legend(loc = 'best')\n",
        "  plt.title('Reconstructed Transactions')\n",
        "  plt.xlabel('Transaction ID')\n",
        "  plt.ylabel('Reconstruction Error')\n",
        "  plt.show();\n",
        "\n",
        "def ModelPerformance(X_test, y_test, prediction):\n",
        "  print('\\n Confusion Matrix \\n =================================== \\n')\n",
        "  sns.heatmap(confusion_matrix(y_test,prediction), annot = True, cmap='RdBu', fmt='g', cbar=False)\n",
        "  plt.title('Confusion Matrix')\n",
        "  plt.ylabel('True Label')\n",
        "  plt.xlabel('Predicted Label')\n",
        "  plt.show();\n",
        "  print('\\n Classification Report \\n =================================== \\n', classification_report(y_test,prediction))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGm5iU-woig5",
        "colab_type": "text"
      },
      "source": [
        "## Sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aaqdgfovonni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Scaling the data\n",
        "\n",
        "df = pd.read_csv('creditcard.csv')\n",
        "scaled = pd.DataFrame(StandardScaler().fit_transform(df.drop(['Time','Class'], axis=1)),\n",
        "                      columns=df.drop(['Time','Class'],axis=1).columns) # scaling all columns apart from Time and Class\n",
        "scaled['Class'] = df['Class']\n",
        "\n",
        "# Splitting the data\n",
        "\n",
        "train, test = train_test_split(scaled, test_size = 0.2, random_state = seed)\n",
        "\n",
        "X_train = (train[train.Class == 0]).drop(['Class'], axis = 1) # using only the cleared transactions for training\n",
        "X_test = test.drop(['Class'], axis=1)\n",
        "y_test = test['Class']\n",
        "\n",
        "input_dim = X_train.shape[1] # input dimensions for the neural networks"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87XwbTd7w1OB",
        "colab_type": "text"
      },
      "source": [
        "## Model 1: Simple Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKeJB8P6w2rW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent = 9 # dimension of the latent vector\n",
        "batch_size = 128\n",
        "epochs = 100\n",
        "\n",
        "# Defining the Model\n",
        "\n",
        "inputs = Input(shape=(input_dim,), name = 'input_encoder')\n",
        "\n",
        "encoder = Dense(18, activation='relu')(inputs)\n",
        "encoder = Dense(12, activation='relu')(encoder)\n",
        "\n",
        "bottleneck  = Dense(latent, activation='relu')(encoder)\n",
        "\n",
        "decoder = Dense(12, activation='relu')(bottleneck)\n",
        "decoder = Dense(18, activation='relu')(decoder)\n",
        "decoder = Dense(input_dim, activation='relu')(decoder)\n",
        "\n",
        "autoencoder_1 = Model(inputs,decoder)\n",
        "# autoencoder_1.summary()\n",
        "# keras.utils.plot_model(autoencoder_1, show_shapes=True)\n",
        "\n",
        "# Compiling the model\n",
        "\n",
        "autoencoder_1.compile(optimizer=keras.optimizers.Adam(),\n",
        "                      loss = losses.mean_squared_error,\n",
        "                      metrics = ['mae','accuracy'])\n",
        "\n",
        "# Early Stopper\n",
        "\n",
        "stopper = EarlyStopping(monitor = 'val_loss',\n",
        "                        min_delta = 0.005,\n",
        "                        verbose = 1,\n",
        "                        patience = 5,\n",
        "                        restore_best_weights = True)\n",
        "\n",
        "# Training\n",
        "\n",
        "history = autoencoder_1.fit(X_train, X_train,\n",
        "                            batch_size = batch_size,\n",
        "                            epochs = epochs,\n",
        "                            verbose = 2,\n",
        "                            validation_data = (X_test, X_test),\n",
        "                            callbacks = [stopper])\n",
        "\n",
        "LossGraph(history)\n",
        "\n",
        "# Predicting\n",
        "\n",
        "predictions = autoencoder_1.predict(X_test)\n",
        "\n",
        "# Reconstruction error (RMSE)\n",
        "\n",
        "rmse = np.sqrt(np.mean(np.power(X_test - predictions, 2), axis = 1))\n",
        "\n",
        "# Reconstructed data\n",
        "\n",
        "reconstructed = pd.DataFrame({'rec_error': rmse,'true_class': y_test})\n",
        "\n",
        "frauds = reconstructed[reconstructed.true_class == 1].drop('true_class', axis = 1)\n",
        "cleared = reconstructed[reconstructed.true_class == 0].drop('true_class', axis = 1)\n",
        "\n",
        "# Predicting the class using the threshtold\n",
        "\n",
        "threshold = 5\n",
        "\n",
        "reconstructed['pred_class'] = 0\n",
        "reconstructed.pred_class[reconstructed.rec_error > threshold] = 1\n",
        "\n",
        "# Model Performance\n",
        "\n",
        "ReconstructionScatter(cleared, frauds, threshold = threshold)\n",
        "ModelPerformance(X_test, y_test, reconstructed.pred_class)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36aaafHeAyaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import time\n",
        "# for i in range(60):\n",
        "#   time.sleep(60)\n",
        "#   print(i)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}